{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f20cf344",
   "metadata": {},
   "source": [
    "# Applying MLP\n",
    "\n",
    "#### 1a: Load and preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d0b07f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages and libraries\n",
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from itertools import combinations\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "854ccf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the datasets\n",
    "file_path = 'data/project_adult.csv'\n",
    "file_path2 = 'data/project_validation_inputs.csv'\n",
    "\n",
    "# converting to dataframes\n",
    "df_adult = pd.read_csv(file_path, header=0, encoding='utf-8')\n",
    "df_validation = pd.read_csv(file_path2, header=0, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9c83b481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0           0\n",
      "age                  0\n",
      "workclass         1447\n",
      "fnlwgt               0\n",
      "education            0\n",
      "education-num        0\n",
      "marital-status       0\n",
      "occupation        1454\n",
      "relationship         0\n",
      "race                 0\n",
      "sex                  0\n",
      "capital-gain         0\n",
      "capital-loss         0\n",
      "hours-per-week       0\n",
      "native-country     458\n",
      "income               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing - handling missing values \n",
    "df_adult.replace('?', np.nan, inplace=True)\n",
    "null_counts = df_adult.isnull().sum()\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f45394a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0          0\n",
      "age                 0\n",
      "workclass         389\n",
      "fnlwgt              0\n",
      "education           0\n",
      "education-num       0\n",
      "marital-status      0\n",
      "occupation        389\n",
      "relationship        0\n",
      "race                0\n",
      "sex                 0\n",
      "capital-gain        0\n",
      "capital-loss        0\n",
      "hours-per-week      0\n",
      "native-country    125\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Make sure the validation set is similar \n",
    "df_validation.replace('?', np.nan, inplace=True)\n",
    "null_counts = df_validation.isnull().sum()\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52de924",
   "metadata": {},
   "source": [
    "This reveals that we need to handle missing data for workclass, occupation, and native-county, which are all categorical. Because they're all categorical, we can use mode imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d1df5198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the dataset: handle missing values, encode categorical features, standardize numerical features, \n",
    "# and separate features from target.\n",
    "def preprocess_data(df, target_column='income', fit_scaler=True, return_split=False, test_size=0.2, random_state=42):\n",
    "    \n",
    "    global trained_scaler\n",
    "    \n",
    "    print(f\"Original data shape: {df.shape}\")\n",
    "    print(f\"Original data columns: {df.columns.tolist()}\")\n",
    "\n",
    "    #####################[ Handle missing values ]#####################\n",
    "\n",
    "    # Convert ? to null for easier handling\n",
    "    df.replace('?', np.nan, inplace=True)\n",
    "\n",
    "    # Use mode imputation to handle missing values in categorical columns\n",
    "    categorical_cols = df.select_dtypes(include=['object','category']).columns\n",
    "    print(f\"Categorical columns: {categorical_cols.tolist()}\\n\\n\")\n",
    "\n",
    "    print(\"MODE IMPUTATION: \\n\")\n",
    "\n",
    "    for col in categorical_cols:\n",
    "        null_count = df[col].isnull().sum()\n",
    "        print(f\"Null count for {col} : {null_count}\")\n",
    "        if null_count > 0:\n",
    "            col_mode = df[col].mode()[0]\n",
    "            df[col].fillna(col_mode, inplace=True)\n",
    "            print(f\"-> Imputed '{col}' with mode: {col_mode}\")\n",
    "\n",
    "\n",
    "    #####################[ Encoding features ]#####################\n",
    "\n",
    "    # Keep track of rows indexes\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # SEPARATE X AND y ONLY IF target_column IS PROVIDED AND EXISTS\n",
    "    if target_column is not None and target_column in df.columns:\n",
    "        y = df[target_column]\n",
    "        X = df.drop(columns=[target_column])\n",
    "        print(f\"\\nAfter separation: \\n\\tX shape: {X.shape} \\n\\ty shape: {y.shape}\")\n",
    "    else:\n",
    "        X = df.copy()\n",
    "        y = None\n",
    "        print(f\"No target column specified or found. Using all columns as features. X shape: {X.shape}\")\n",
    "    \n",
    "    # Encode categorical features (only on X, not y)    \n",
    "    # One-hot encoding\n",
    "    cols_to_encode = [col for col in categorical_cols if col in X.columns]\n",
    "    X = pd.get_dummies(X, columns=cols_to_encode, drop_first=True)\n",
    "    \n",
    "    print(f\"After encoding categorical features X shape: {X.shape}\")    \n",
    "\n",
    "    # Ensure all columns are numeric\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype == 'object':\n",
    "            X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "\n",
    "\n",
    "    #####################[ Standardization ]#####################\n",
    "\n",
    "    # Global scaling for ALL features\n",
    "    if fit_scaler:\n",
    "        final_scaler = StandardScaler()\n",
    "        X = pd.DataFrame(final_scaler.fit_transform(X), columns=X.columns)\n",
    "    \n",
    "    \n",
    "    #####################[ Train/test split ]#####################\n",
    "\n",
    "    if return_split and y is not None:\n",
    "        # Split into train and test sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "        )\n",
    "        print(f\"Train/test split: \\n\\tX_train: {X_train.shape} \\n\\tX_test: {X_test.shape}\")\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    elif return_split and y is None:\n",
    "        raise ValueError(\"Cannot return split when no target column is available\")\n",
    "    else:\n",
    "        if y is not None:\n",
    "            return X, y\n",
    "        else:\n",
    "            return X  # Return only X if no target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "fe9a6355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (26048, 16)\n",
      "Original data columns: ['Unnamed: 0', 'age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']\n",
      "Categorical columns: ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country', 'income']\n",
      "\n",
      "\n",
      "MODE IMPUTATION: \n",
      "\n",
      "Null count for workclass : 1447\n",
      "-> Imputed 'workclass' with mode: Private\n",
      "Null count for education : 0\n",
      "Null count for marital-status : 0\n",
      "Null count for occupation : 1454\n",
      "-> Imputed 'occupation' with mode: Prof-specialty\n",
      "Null count for relationship : 0\n",
      "Null count for race : 0\n",
      "Null count for sex : 0\n",
      "Null count for native-country : 458\n",
      "-> Imputed 'native-country' with mode: United-States\n",
      "Null count for income : 0\n",
      "\n",
      "After separation: \n",
      "\tX shape: (26048, 15) \n",
      "\ty shape: (26048,)\n",
      "After encoding categorical features X shape: (26048, 98)\n",
      "Train/test split: \n",
      "\tX_train: (20838, 98) \n",
      "\tX_test: (5210, 98)\n"
     ]
    }
   ],
   "source": [
    "# Preprocess and split data\n",
    "X_train, X_test, y_train, y_test = preprocess_data(\n",
    "    df_adult, \n",
    "    target_column='income', \n",
    "    fit_scaler=True, \n",
    "    return_split=True,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d45c55f",
   "metadata": {},
   "source": [
    "#### 1b: Train and test MLP model on dataset while tunning parameters to develop multiple candidates\n",
    "#### 1c: Evaluate models using appropriate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79354d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geoenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
